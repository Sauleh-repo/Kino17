{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1X0hJvZbO7-Vs_Tg-S6hhxtH9qcv222_f",
      "authorship_tag": "ABX9TyOOyZFyqDWCIdsw9Imuhv3t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sauleh-repo/CIT/blob/main/ViT(UPAD).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d mahdavi1202/skin-cancer\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "with zipfile.ZipFile('skin-cancer.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('skin-cancer')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "Wra-FGTi8z7O",
        "outputId": "bd9d320e-da81-4890-8553-6fe357043857"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4cf29c82-71cb-4afc-b532-7d77fd681872\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4cf29c82-71cb-4afc-b532-7d77fd681872\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n",
            "Dataset URL: https://www.kaggle.com/datasets/mahdavi1202/skin-cancer\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "skin-cancer.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('skin-cancer')\n",
        "import pandas as pd\n",
        "import shutil\n",
        "os.mkdir(\"/content/all_images\")\n",
        "source_dir=\"skin-cancer/imgs_part_1/imgs_part_1\"\n",
        "file_names = os.listdir(source_dir)\n",
        "for file_name in file_names:\n",
        "    shutil.move(os.path.join(source_dir, file_name), \"/content/all_images\")\n",
        "\n",
        "source_dir=\"skin-cancer/imgs_part_2/imgs_part_2\"\n",
        "file_names = os.listdir(source_dir)\n",
        "for file_name in file_names:\n",
        "    shutil.move(os.path.join(source_dir, file_name), \"/content/all_images\")\n",
        "\n",
        "source_dir=\"skin-cancer/imgs_part_3/imgs_part_3\"\n",
        "file_names = os.listdir(source_dir)\n",
        "for file_name in file_names:\n",
        "    shutil.move(os.path.join(source_dir, file_name), \"/content/all_images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "iUbXRR06DrPs",
        "outputId": "40c044c0-0583-424f-d1c7-aed40a54f4c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/content/all_images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-318143eec2f5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/all_images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msource_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skin-cancer/imgs_part_1/imgs_part_1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/all_images'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"skin-cancer/metadata.csv\")\n",
        "data['full_link'] = 'all_images/' + data['img_id']\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax1 = plt.subplots(1, 1, figsize= (10, 5))\n",
        "data['diagnostic'].value_counts().plot(kind='bar', ax=ax1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "oc7K_6shLVSF",
        "outputId": "8dd432f0-7f78-4dd9-c407-402c3c7b29d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='diagnostic'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diagnostic_classes = {0:'BCC', 1 : 'ACK', 2 : 'NEV', 3 : 'SEK', 4 : 'SCC', 5: 'MEL'}\n",
        "\n",
        "# a function for encoding classes\n",
        "def create_class(X):\n",
        "    if X == 'BCC':\n",
        "        return 0\n",
        "    elif X =='ACK':\n",
        "        return 1\n",
        "    elif X == 'NEV':\n",
        "        return 2\n",
        "    elif X == 'SEK':\n",
        "        return 3\n",
        "    elif X == 'SCC':\n",
        "        return 4\n",
        "    elif X == 'MEL':\n",
        "        return 5\n",
        "    else:\n",
        "        print('error class')"
      ],
      "metadata": {
        "id": "T_lF7bI1Sf7S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU0eP9t4LtZ-",
        "outputId": "b8e1bed3-21c8-45b8-fb84-6e537efb4440"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2298 entries, 0 to 2297\n",
            "Data columns (total 27 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   patient_id           2298 non-null   object \n",
            " 1   lesion_id            2298 non-null   int64  \n",
            " 2   smoke                1494 non-null   object \n",
            " 3   drink                1494 non-null   object \n",
            " 4   background_father    1480 non-null   object \n",
            " 5   background_mother    1476 non-null   object \n",
            " 6   age                  2298 non-null   int64  \n",
            " 7   pesticide            1494 non-null   object \n",
            " 8   gender               1494 non-null   object \n",
            " 9   skin_cancer_history  1494 non-null   object \n",
            " 10  cancer_history       1494 non-null   object \n",
            " 11  has_piped_water      1494 non-null   object \n",
            " 12  has_sewage_system    1494 non-null   object \n",
            " 13  fitspatrick          1494 non-null   float64\n",
            " 14  region               2298 non-null   object \n",
            " 15  diameter_1           1494 non-null   float64\n",
            " 16  diameter_2           1494 non-null   float64\n",
            " 17  diagnostic           2298 non-null   object \n",
            " 18  itch                 2298 non-null   object \n",
            " 19  grew                 2298 non-null   object \n",
            " 20  hurt                 2298 non-null   object \n",
            " 21  changed              2298 non-null   object \n",
            " 22  bleed                2298 non-null   object \n",
            " 23  elevation            2298 non-null   object \n",
            " 24  img_id               2298 non-null   object \n",
            " 25  biopsed              2298 non-null   bool   \n",
            " 26  full_link            2298 non-null   object \n",
            "dtypes: bool(1), float64(3), int64(2), object(21)\n",
            "memory usage: 469.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data['encoded_class'] = data['diagnostic'].apply(create_class)\n",
        "data.drop(['diagnostic'], axis = 1, inplace = True)\n",
        "data.sort_values(by ='patient_id', ascending = True, inplace = True, ignore_index = True)\n",
        "data.info()\n",
        "data.drop([ 'biopsed','patient_id','img_id','lesion_id','smoke', 'drink', 'background_father', 'background_mother', 'pesticide', 'gender', 'skin_cancer_history',\n",
        "         'cancer_history', 'has_piped_water', 'has_sewage_system', 'fitspatrick', 'diameter_1', 'diameter_2'], axis = 1, inplace = True)\n",
        "data.info()\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "IMG_SIZE = 32,32\n",
        "BATCH_SIZE = 256\n",
        "SEED = 55\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "train_data = data[:2000]\n",
        "test_data = data[2000:]\n",
        "test_data = shuffle(test_data, random_state = SEED).reset_index(drop = True)\n",
        "\n",
        "print('train  ->', train_data.shape)\n",
        "print('test  ->', test_data.shape)\n",
        "counts = np.bincount(train_data['encoded_class'])\n",
        "\n",
        "weight_for_0 = 1.0 / counts[0]\n",
        "weight_for_1 = 1.0 / counts[1]\n",
        "weight_for_2 = 1.0 / counts[2]\n",
        "weight_for_3 = 1.0 / counts[3]\n",
        "weight_for_4 = 1.0 / counts[4]\n",
        "weight_for_5 = 1.0 / counts[5]\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3, 4: weight_for_4, 5: weight_for_5}\n",
        "class_weight\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class ClassToken(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.cls = self.add_weight(             #adding a trainable paramter to a custom layer\n",
        "            name=\"cls\",                         #name of the weight\n",
        "            shape=(1, 1, input_shape[-1]),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        cls = tf.tile(self.cls, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls, x], axis=1)\n",
        "        return x\n",
        "\n",
        "def mlp(x, cf):\n",
        "    x = Dense(cf[\"mlp_dim\"], activation=\"gelu\")(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    x = Dense(cf[\"hidden_dim\"])(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    return x\n",
        "\n",
        "def transformer_encoder(x, cf):\n",
        "    skip_1 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = MultiHeadAttention(\n",
        "        num_heads=cf[\"num_heads\"], key_dim=cf[\"hidden_dim\"]\n",
        "    )(x, x)\n",
        "    x = Add()([x, skip_1])\n",
        "\n",
        "    skip_2 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = mlp(x, cf)\n",
        "    x = Add()([x, skip_2])\n",
        "\n",
        "    return x\n",
        "\n",
        "def ViT(cf):\n",
        "\n",
        "    input_shape = (cf[\"num_patches\"], cf[\"patch_size\"]*cf[\"patch_size\"]*cf[\"num_channels\"])\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    patch_embed = Dense(cf[\"hidden_dim\"])(inputs)\n",
        "\n",
        "    positions = tf.range(start=0, limit=cf[\"num_patches\"], delta=1)\n",
        "    pos_embed = Embedding(input_dim=cf[\"num_patches\"], output_dim=cf[\"hidden_dim\"])(positions)\n",
        "    embed = patch_embed + pos_embed\n",
        "\n",
        "\n",
        "    x = ClassToken()(embed)\n",
        "\n",
        "    for _ in range(cf[\"num_layers\"]):\n",
        "        x = transformer_encoder(x, cf)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = x[:, 0, :]\n",
        "    x = Dense(cf[\"num_classes\"], activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6K-lgVU879B",
        "outputId": "fe446b69-b9c0-448c-b4d0-4aa788037c13"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2298 entries, 0 to 2297\n",
            "Data columns (total 27 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   patient_id           2298 non-null   object \n",
            " 1   lesion_id            2298 non-null   int64  \n",
            " 2   smoke                1494 non-null   object \n",
            " 3   drink                1494 non-null   object \n",
            " 4   background_father    1480 non-null   object \n",
            " 5   background_mother    1476 non-null   object \n",
            " 6   age                  2298 non-null   int64  \n",
            " 7   pesticide            1494 non-null   object \n",
            " 8   gender               1494 non-null   object \n",
            " 9   skin_cancer_history  1494 non-null   object \n",
            " 10  cancer_history       1494 non-null   object \n",
            " 11  has_piped_water      1494 non-null   object \n",
            " 12  has_sewage_system    1494 non-null   object \n",
            " 13  fitspatrick          1494 non-null   float64\n",
            " 14  region               2298 non-null   object \n",
            " 15  diameter_1           1494 non-null   float64\n",
            " 16  diameter_2           1494 non-null   float64\n",
            " 17  itch                 2298 non-null   object \n",
            " 18  grew                 2298 non-null   object \n",
            " 19  hurt                 2298 non-null   object \n",
            " 20  changed              2298 non-null   object \n",
            " 21  bleed                2298 non-null   object \n",
            " 22  elevation            2298 non-null   object \n",
            " 23  img_id               2298 non-null   object \n",
            " 24  biopsed              2298 non-null   bool   \n",
            " 25  full_link            2298 non-null   object \n",
            " 26  encoded_class        2298 non-null   int64  \n",
            "dtypes: bool(1), float64(3), int64(3), object(20)\n",
            "memory usage: 469.2+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2298 entries, 0 to 2297\n",
            "Data columns (total 10 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   age            2298 non-null   int64 \n",
            " 1   region         2298 non-null   object\n",
            " 2   itch           2298 non-null   object\n",
            " 3   grew           2298 non-null   object\n",
            " 4   hurt           2298 non-null   object\n",
            " 5   changed        2298 non-null   object\n",
            " 6   bleed          2298 non-null   object\n",
            " 7   elevation      2298 non-null   object\n",
            " 8   full_link      2298 non-null   object\n",
            " 9   encoded_class  2298 non-null   int64 \n",
            "dtypes: int64(2), object(8)\n",
            "memory usage: 179.7+ KB\n",
            "train  -> (2000, 10)\n",
            "test  -> (298, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_positional_embedding(num_patches, embedding_dim):\n",
        "    return tf.random.normal(shape=(1,num_patches,embedding_dim))\n",
        "\n",
        "def preprocess_image(image, patch_size, target_size=(224, 224)):\n",
        "    if image.shape[-1] == 1:\n",
        "        image = tf.image.grayscale_to_rgb(image)\n",
        "\n",
        "    image = tf.image.resize(image, target_size)\n",
        "\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=tf.expand_dims(image, axis=0),\n",
        "        sizes=[1, patch_size, patch_size, 1],\n",
        "        strides=[1, patch_size, patch_size, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding='VALID'\n",
        "    )\n",
        "    num_patches = (target_size[0] // patch_size) * (target_size[1] // patch_size)\n",
        "    patch_dim = patch_size * patch_size * 3\n",
        "    patches = tf.reshape(patches, (num_patches, patch_dim))\n",
        "\n",
        "    return patches\n",
        "image = tf.random.normal(shape=(28,28,3))\n",
        "\n",
        "patch_size = 16\n",
        "processed_patches = preprocess_image(image, patch_size)\n",
        "print(\"Shape of processed patches:\", processed_patches.shape)\n",
        "\n",
        "patch_size = 32\n",
        "processed_patches = preprocess_image(image, patch_size)\n",
        "print(\"Shape of processed patches:\", processed_patches.shape)\n",
        "image = tf.random.normal(shape=(100,150,3))\n",
        "\n",
        "patch_size = 16\n",
        "processed_patches = preprocess_image(image, patch_size)\n",
        "print(\"Shape of processed patches:\", processed_patches.shape)\n",
        "\n",
        "patch_size = 32\n",
        "processed_patches = preprocess_image(image, patch_size)\n",
        "print(\"Shape of processed patches:\", processed_patches.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNwbGj9kgip3",
        "outputId": "ae1baa44-111a-468f-db7b-0c5a8de58f2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of processed patches: (196, 768)\n",
            "Shape of processed patches: (49, 3072)\n",
            "Shape of processed patches: (196, 768)\n",
            "Shape of processed patches: (49, 3072)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "  images, labels = [], []\n",
        "  for index, row in data.iterrows():\n",
        "    image_path = row['full_link']\n",
        "    image = tf.keras.utils.load_img(image_path, target_size=IMG_SIZE)\n",
        "    image = tf.keras.utils.img_to_array(image)\n",
        "\n",
        "    processed_image = preprocess_image(image, patch_size=cf[\"patch_size\"])\n",
        "    images.append(processed_image)\n",
        "    labels.append(row['encoded_class'])\n",
        "\n",
        "  return np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "S1j5HLzFlaws"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_images, train_labels = preprocess_data(train_data)\n",
        "\n",
        "test_images, test_labels = preprocess_data(test_data)\n"
      ],
      "metadata": {
        "id": "r7dJV2kPlcR9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 55\n",
        "cf = {\n",
        "    \"patch_size\": 16,\n",
        "    \"num_patches\": (IMG_SIZE[0] // 16) * (IMG_SIZE[1] // 16),\n",
        "    \"num_channels\": 3,\n",
        "    \"hidden_dim\": 768,\n",
        "    \"mlp_dim\": 3072,\n",
        "    \"num_heads\": 12,\n",
        "    \"num_layers\": 12,\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"num_classes\": 6,\n",
        "    \"weight_decay\": 1e-6,\n",
        "    \"learning_rate\": 1e-3\n",
        "}\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "model = ViT(cf)\n",
        "\n",
        "optimizer = Adam(learning_rate=cf[\"learning_rate\"], weight_decay=cf[\"weight_decay\"])\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,  # Adjust the number of epochs as needed\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    class_weight=class_weight\n",
        ")\n",
        "\n",
        "# Evaluation\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McqVFkWDl1wv",
        "outputId": "88f87c67-1ea7-48c9-dc6b-107a34fcc968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Previously working\n"
      ],
      "metadata": {
        "id": "y7dI8LHMZhX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_image(image_path, label):\n",
        "    \"\"\"Loads an image, preprocesses it, and returns (image, label).\"\"\"\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)  # Ensure the image is 3D\n",
        "    img = tf.image.resize(img, IMG_SIZE)  # Resize to target size (e.g., 32x32)\n",
        "    img = tf.cast(img, tf.float32) / 255.0  # Normalize pixel values\n",
        "    return img, label\n",
        "\n",
        "def augment(image, label):\n",
        "    \"\"\"Applies data augmentation.\"\"\"\n",
        "    # Apply augmentation on the 3D image\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)  # Adjust brightness\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)  # Adjust contrast\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "zbrhwaGXEWbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image, patch_size):\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=tf.expand_dims(image, axis=0),  # Add batch dimension\n",
        "        sizes=[1, patch_size, patch_size, 1],\n",
        "        strides=[1, patch_size, patch_size, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding=\"VALID\"\n",
        "    )\n",
        "    patches = tf.reshape(patches, [-1, patch_size * patch_size * 3])  # Flatten patches\n",
        "    return patches\n",
        "\n",
        "train_image_paths = train_data['full_link'].values\n",
        "train_labels = train_data['encoded_class'].values\n",
        "\n",
        "test_image_paths = test_data['full_link'].values\n",
        "test_labels = test_data['encoded_class'].values\n",
        "\n",
        "train_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((train_image_paths, train_labels))\n",
        "    .map(load_and_preprocess_image, num_parallel_calls=AUTO)\n",
        "    .map(augment, num_parallel_calls=AUTO)  # Apply augmentation\n",
        "    .map(lambda img, lbl: (preprocess_image(img, cf[\"patch_size\"]), lbl), num_parallel_calls=AUTO)  # Extract patches\n",
        "    .shuffle(buffer_size=len(train_image_paths))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((test_image_paths, test_labels))\n",
        "    .map(load_and_preprocess_image, num_parallel_calls=AUTO)\n",
        "    .map(lambda img, lbl: (preprocess_image(img, cf[\"patch_size\"]), lbl), num_parallel_calls=AUTO)  # Extract patches\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "# Model Compilation\n",
        "cf = {\n",
        "    \"num_patches\": (IMG_SIZE[0] // cf[\"patch_size\"]) * (IMG_SIZE[1] // cf[\"patch_size\"]),\n",
        "    \"patch_size\": 4,\n",
        "    \"num_channels\": 3,\n",
        "    \"hidden_dim\": 64,\n",
        "    \"mlp_dim\": 128,\n",
        "    \"num_heads\": 4,\n",
        "    \"num_layers\": 4,\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"num_classes\": 6,\n",
        "}\n",
        "\n",
        "model = ViT(cf)\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "#Training\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=test_dataset,\n",
        "    class_weight=class_weight\n",
        ")\n",
        "\n",
        "#Evaluating\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "model.save('skin_cancer_vit_model.h5')\n",
        "\n",
        "# Example Prediction Function\n",
        "def predict_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=tf.expand_dims(img, axis=0),  # Add batch dimension for prediction\n",
        "        sizes=[1, cf[\"patch_size\"], cf[\"patch_size\"], 1],\n",
        "        strides=[1, cf[\"patch_size\"], cf[\"patch_size\"], 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding=\"VALID\"\n",
        "    )\n",
        "    patches = tf.reshape(patches, [-1, cf[\"patch_size\"] * cf[\"patch_size\"] * 3])\n",
        "    patches = tf.expand_dims(patches, axis=0)  # Add batch dimension for ViT input\n",
        "\n",
        "    prediction = model.predict(patches)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    return predicted_class\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0EvfGgfzDw-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "sample_image_path = test_image_paths[15]\n",
        "predicted_class = predict_image(sample_image_path)\n",
        "print(f\"Predicted class for {sample_image_path}: {predicted_class}\")"
      ],
      "metadata": {
        "id": "hNQluLJiJhMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification Report"
      ],
      "metadata": {
        "id": "OvThRcK7yMH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 55\n",
        "cf = {\n",
        "    \"patch_size\": 16,  #Experiment with 16 or 32\n",
        "    \"num_patches\": (IMG_SIZE[0] // 16) * (IMG_SIZE[1] // 16),\n",
        "    \"num_channels\": 3,\n",
        "    \"hidden_dim\": 768,\n",
        "    \"mlp_dim\": 3072,\n",
        "    \"num_heads\": 12,\n",
        "    \"num_layers\": 12,\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"num_classes\": 6,\n",
        "    \"weight_decay\": 1e-6,\n",
        "    \"learning_rate\": 1e-3\n",
        "}\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "model = ViT(cf)\n",
        "model.summary()\n",
        "\n",
        "optimizer = Adam(learning_rate=cf[\"learning_rate\"], weight_decay=cf[\"weight_decay\"])\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,  # Adjust the number of epochs as needed\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    class_weight=class_weight\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification Report and Confusion Matrix\n",
        "predictions = model.predict(test_dataset)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(classification_report(test_labels, predicted_labels))\n",
        "\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "#Plotting training history\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "remove all the plots and redundancy. only focus on training the model and calculating the accuracy of our model"
      ],
      "metadata": {
        "id": "iQK4Yei9yKC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}